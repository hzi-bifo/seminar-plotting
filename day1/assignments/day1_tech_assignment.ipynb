{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1: Introduction to Data Analysis with Python\n",
        "\n",
        "Welcome to your first day of data analysis! In this notebook, we'll explore the fundamentals of working with data using Python's most powerful libraries for data science.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this session, you will be able to:\n",
        "- Import and use essential data science libraries\n",
        "- Load datasets from CSV files\n",
        "- Understand the difference between wide and long data formats\n",
        "- Transform data between different formats using pandas\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Step 1: Importing Essential Libraries\n",
        "\n",
        "Before we can work with data, we need to import the libraries that give us the tools we need.\n",
        "\n",
        "To do so, use\n",
        "```python\n",
        "import library_name as alias\n",
        "```\n",
        "\n",
        "Think of these as specialized toolkits:\n",
        "\n",
        "- **pandas** (`pd`): The powerhouse for data manipulation and analysis. It provides DataFrames, which are like supercharged spreadsheets in Python.\n",
        "- **matplotlib.pyplot** (`plt`): A comprehensive library for creating static, animated, and interactive visualizations.\n",
        "- **seaborn** (`sns`): Built on top of matplotlib, it provides a high-level interface for drawing attractive statistical graphics.\n",
        "\n",
        "### Why Use Aliases?\n",
        "\n",
        "We use short aliases (like `pd`, `plt`, `sns`) to make our code cleaner and follow community conventions. This is a standard practice that makes your code more readable and easier to share with other data scientists.\n",
        "\n",
        "**Best Practice:** Always import these libraries at the very beginning of your notebook!"
      ],
      "metadata": {
        "id": "NhNWHFFDBhvU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IIROf76sPRaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Step 2: Loading Your First Dataset\n",
        "Now that we have our tools ready, it's time to load some real data. We'll be working with the Palmer Penguins dataset,\n",
        "\n",
        "\n",
        "```python\n",
        "data =  pd.read_csv('penguins.csv', sep= ',' ,header = 0)\n",
        "```\n",
        "The read_csv() function is one of the most commonly used pandas functions.\n",
        "\n",
        "Let's break down the parameters:\n",
        "\n",
        "'penguins.csv': The filename or path to your CSV file\n",
        "sep=',': The delimiter character (comma in this case, which is standard for CSV files)\n",
        "header=0: Tells pandas that the first row (index 0) contains column names\n",
        "Pro Tip\n",
        "Simply typing the DataFrame variable name (like df) at the end of a cell will display the data in a nice, formatted table. This is one of the convenient features of Jupyter notebooks!"
      ],
      "metadata": {
        "id": "6BJe4gnoUj1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0rmiwNBhPQwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Inspect data\n",
        "Use .columns on your dataframe to see the columns of the dataset.\n",
        "\n",
        "Before diving into data analysis, it's crucial to understand what information your dataset contains. The `.columns` attribute is one of the first tools you should use when exploring a new dataset.\n",
        "\n",
        "**What does `.columns` do?**\n",
        "- Returns an Index object containing all column names in your DataFrame\n",
        "- Shows you exactly what variables you have available for analysis\n",
        "- Helps you understand the structure of your data\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.columns\n",
        "```\n",
        "\n",
        "**Pro Tip:** You can convert this to a list for easier reading:\n",
        "```python\n",
        "list(df.columns)\n",
        "```"
      ],
      "metadata": {
        "id": "ZS49IBLtVJvV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MmWrBoCMPSwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the first questions you should ask when working with a dataset is: \"How big is it?\" The `.shape` attribute gives you a quick answer by returning the dimensions of your DataFrame.\n",
        "\n",
        "**What does `.shape` do?**\n",
        "- Returns a tuple with two values: (number of rows, number of columns)\n",
        "- Provides an instant overview of your dataset's size\n",
        "- Helps you understand the scale of data you're working with\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.shape\n",
        "```\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "1. **Memory Management**: Know if you're working with a small dataset (hundreds of rows) or big data (millions of rows)\n",
        "2. **Performance Planning**: Large datasets may require different approaches or more processing time\n",
        "3. **Data Validation**: Verify that your data loaded correctly (e.g., if you expect 1000 rows but only see 100, something went wrong)\n",
        "4. **Quick Context**: Understand the scope of your analysis before diving in\n",
        "\n",
        "**Understanding the Output:**\n",
        "- **First number (rows)**: The number of observations/records in your dataset\n",
        "- **Second number (columns)**: The number of variables/features you have\n",
        "\n",
        "**Pro Tip:** You can access individual dimensions:\n",
        "```python\n",
        "num_rows = df.shape[0]      # Get number of rows\n",
        "num_cols = df.shape[1]      # Get number of columns\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "qfG-h3krCGxY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2sIWy3azPUEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Previewing Your Data with `.head()` and `.tail()`\n",
        "\n",
        "### Using `df.head()` to View the First Rows\n",
        "\n",
        "After understanding your dataset's size and column names, the next step is to actually look at the data. The `.head()` method gives you a quick peek at the beginning of your dataset.\n",
        "\n",
        "**What does `.head()` do?**\n",
        "- Displays the first few rows of your DataFrame\n",
        "- By default, shows 5 rows\n",
        "- Allows you to customize how many rows to display\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.head()          # Shows first 5 rows (default)\n",
        "df.head(10)        # Shows first 10 rows\n",
        "df.head(3)         # Shows first 3 rows\n",
        "```\n",
        "\n",
        "\n",
        "**When to use different numbers:**\n",
        "- Use `head(3)` for a quick glance\n",
        "- Use `head(10)` or `head(20)` for a more thorough initial inspection\n",
        "- Use `head(100)` when you need to see more patterns or variations\n"
      ],
      "metadata": {
        "id": "K9Lh741HCDs8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEzHAEOpPVQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.tail()` works the same way as .head() but shows the **last** rows of your dataset, which is useful for checking if data loaded completely.\n",
        "\n",
        "Try using `.tail()` with different numbers to explore the penguin dataset!"
      ],
      "metadata": {
        "id": "M3j_7v91CRir"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bzWXTfyvPZ_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistical Summary with `.describe()`\n",
        "\n",
        "### Using `.describe()` to Get Quick Statistics\n",
        "\n",
        "Once you've previewed your data, the next crucial step is understanding its statistical properties. The `.describe()` method is your go-to tool for getting a comprehensive statistical summary of your dataset.\n",
        "\n",
        "**What does `.describe()` do?**\n",
        "- Generates descriptive statistics for all numerical columns in your DataFrame\n",
        "- Provides a quick overview of central tendency, dispersion, and distribution\n",
        "- Automatically excludes non-numeric columns (like text or categories) by default\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.describe()                    # Statistics for numeric columns only\n",
        "df.describe(include='all')       # Statistics for all columns (numeric and categorical)\n",
        "df.describe(include=['object'])  # Statistics for categorical columns only\n",
        "```\n",
        "\n",
        "**Key Statistics Provided:**\n",
        "\n",
        "For **numeric columns**, you get:\n",
        "- **count**: Number of non-null (non-missing) values\n",
        "- **mean**: Average value\n",
        "- **std**: Standard deviation (measure of spread)\n",
        "- **min**: Minimum value\n",
        "- **25%**: First quartile (25th percentile)\n",
        "- **50%**: Median (middle value, 50th percentile)\n",
        "- **75%**: Third quartile (75th percentile)\n",
        "- **max**: Maximum value\n",
        "\n",
        "\n",
        "\n",
        "**Pro Tip:** Use `.describe().T` to transpose the output, making it easier to read when you have many columns!\n",
        "\n",
        "Run `.describe()` on the penguin dataset to get a statistical overview of the measurements!"
      ],
      "metadata": {
        "id": "bw8RfteDChXc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ahtR6G3MPbXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Overview with `.info()`\n",
        "\n",
        "### Using `.info()` to Get a Complete Data Summary\n",
        "\n",
        "While `.describe()` gives you statistics, `.info()` provides a structural overview of your DataFrame. This method is essential for understanding the technical details of your dataset.\n",
        "\n",
        "**What does `.info()` do?**\n",
        "- Displays the DataFrame's structure and metadata\n",
        "- Shows column names, data types, and memory usage\n",
        "- Reports the count of non-null (non-missing) values for each column\n",
        "- Provides a quick diagnostic of data completeness\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.info()\n",
        "```\n",
        "\n",
        "**Key Information Provided:**\n",
        "\n",
        "1. **RangeIndex**: Total number of rows and the index range\n",
        "2. **Column Names**: All column names in order\n",
        "3. **Non-Null Count**: How many valid (non-missing) entries each column has\n",
        "4. **Dtype**: Data type of each column (int64, float64, object, etc.)\n",
        "5. **Memory Usage**: How much RAM your DataFrame is consuming\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "1. **Data Type Verification**: Ensure columns have the correct type (dates as datetime, numbers as numeric, not text)\n",
        "2. **Missing Data Detection**: Instantly identify which columns have missing values and how many\n",
        "3. **Memory Management**: Understand memory consumption, especially important for large datasets\n",
        "4. **Data Cleaning Planning**: Quickly spot which columns need attention before analysis\n",
        "5. **Type Conversion Needs**: Identify when you need to convert data types (e.g., object to numeric)"
      ],
      "metadata": {
        "id": "AT7RFn4FCoxz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_cz-1FQPcp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data types"
      ],
      "metadata": {
        "id": "L86D78RWYpIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Type data.body_mass_g. This will return one column showing the values and the type of the values in the end as dtype. Sometimes this is desired to change"
      ],
      "metadata": {
        "id": "JnPtNfN7C4jh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j6T8I4TIPfbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting Data Types with `.astype()`\n",
        "\n",
        "### Changing Column Data Types\n",
        "\n",
        "Sometimes pandas doesn't automatically assign the correct data type to your columns, or you need to convert data for specific analyses. The `.astype()` method allows you to explicitly change a column's data type.\n",
        "\n",
        "**What does `.astype()` do?**\n",
        "- Converts a column from one data type to another\n",
        "- Forces pandas to interpret the data in a specific way\n",
        "- Can handle various type conversions (numeric, string, categorical, etc.)\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.column_name.astype('Int64')\n",
        "```\n",
        "\n",
        "**Understanding 'Int64' vs 'int64':**\n",
        "\n",
        "This is a crucial distinction that many beginners miss!\n",
        "\n",
        "**`int64` (lowercase):**\n",
        "- Standard integer type in NumPy/pandas\n",
        "- **Cannot handle missing values (NaN)**\n",
        "- If a column has any NaN values, conversion will fail or convert to float64\n",
        "- Fast and memory-efficient\n",
        "\n",
        "**`Int64` (uppercase - note the capital I):**\n",
        "- Nullable integer type introduced in pandas\n",
        "- **Can handle missing values while remaining an integer type**\n",
        "- Missing values are represented as `<NA>` instead of NaN\n",
        "- Slightly more memory overhead but much more flexible\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "1. **Preserve Data Integrity**: Keep integer columns as integers even with missing values\n",
        "2. **Accurate Calculations**: Ensure mathematical operations work correctly\n",
        "3. **Proper Aggregations**: Avoid issues when grouping or summarizing data\n",
        "4. **Type Safety**: Prevent unexpected behavior in analysis\n",
        "\n",
        "**What to Observe in the dtype Field:**\n",
        "\n",
        "After running the conversion, look at the dtype field:\n",
        "- **Before conversion**: Might show `float64` (because of NaN values) or `object` (if mixed types)\n",
        "- **After conversion to Int64**: Shows `Int64`\n",
        "- **Key observation**: The capital 'I' indicates this is a nullable integer type that can coexist with missing values\n",
        "\n",
        "**Common Type Conversions:**\n",
        "```python\n",
        "df.column_name.astype('float64')    # Convert to decimal numbers\n",
        "df.column_name.astype('str')        # Convert to text/string\n",
        "df.column_name.astype('datetime64') # Convert to date/time\n",
        "df.column_name.astype('bool')       # Convert to True/False\n",
        "```\n",
        "\n",
        "**Handling Conversion Errors:**\n",
        "```python\n",
        "df.column_name.astype('Int64', errors='ignore')  # Skip if conversion fails\n",
        "df.column_name.astype('Int64', errors='raise')   # Raise error if fails (default)\n",
        "```\n",
        "\n",
        "**Pro Tip:** Always check your data types with `df.dtypes` or `df.info()` after loading data. Incorrect types are a common source of bugs in data analysis!\n",
        "\n",
        "Try converting a numeric column to 'Int64' and observe how the dtype changes from float64 to Int64, allowing integers to coexist with missing values"
      ],
      "metadata": {
        "id": "gAkM9VsCDZsR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pU2jqyUcPgmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also the option of downcast\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "FsAh5FY_DlZS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4WRDXH1rPig_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling NA values"
      ],
      "metadata": {
        "id": "1SwsH01VVVNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting Missing Values with `.isnull().sum()`\n",
        "\n",
        "### Using `.isnull().sum()` to Count Missing Data\n",
        "\n",
        "Missing data is one of the most common challenges in data analysis. Before you can analyze or visualize your data, you need to know where the gaps are. The `.isnull().sum()` method provides a clear count of missing values in each column.\n",
        "\n",
        "**What does `.isnull().sum()` do?**\n",
        "- Identifies all null/missing values (NaN) in your DataFrame\n",
        "- Counts how many missing values exist in each column\n",
        "- Returns a Series showing the count for every column\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df.isnull().sum()          # Count of null values per column\n",
        "df.isnull().sum().sum()    # Total null values in entire DataFrame\n",
        "```\n",
        "\n",
        "**How it works (step by step):**\n",
        "1. **`.isnull()`**: Creates a DataFrame of True/False values (True where data is missing)\n",
        "2. **`.sum()`**: Counts the True values (missing entries) for each column\n",
        "\n",
        "**Why is this important?**\n",
        "\n",
        "1. **Data Quality Assessment**: Understand the completeness of your dataset\n",
        "2. **Analysis Planning**: Decide how to handle missing data before proceeding\n",
        "3. **Pattern Recognition**: Some columns might have more missing values than others\n",
        "4. **Decision Making**: Determine if you should drop rows, fill values, or keep them as-is\n",
        "5. **Avoid Errors**: Many statistical functions fail or give incorrect results with missing data\n",
        "\n",
        "\n",
        "**Complementary Functions:**\n",
        "```python\n",
        "df.isnull().any()          # Boolean: Does each column have ANY missing values?\n",
        "df.isnull().sum() / len(df) # Percentage of missing values per column\n",
        "```"
      ],
      "metadata": {
        "id": "Ng1M65aLDppX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqNpzfVRPko2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Printing the first 10 lines (remember! .head()) we see some of them"
      ],
      "metadata": {
        "id": "rcAikBkvEHqH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SzPWOlTmPmkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Missing Data with `.dropna()`\n",
        "\n",
        "### Removing Columns with Too Many Missing Values\n",
        "\n",
        "After identifying missing values, the next step is deciding what to do with them. The `.dropna()` method gives you powerful options for removing missing data based on specific criteria.\n",
        "\n",
        "**Understanding the Code:**\n",
        "\n",
        "```python\n",
        "df_drop = df.dropna(axis=1, thresh= 0)\n",
        "```\n",
        "\n",
        "**Breaking Down the Parameters:**\n",
        "\n",
        "**`axis=1`**:\n",
        "- To use **columns**, use axis= 1\n",
        "\n",
        "**`thresh=340`**:\n",
        "- **Threshold**: Minimum number of non-null values required to keep a column\n",
        "\n",
        "**Add print() Statements:** print()\n",
        "- **Before**: To show original DataFrame dimensions (rows, columns), using df.shape\n",
        "- **After**: To show dimensions after dropping columns with too many missing values, use df.shape again\n",
        "\n",
        "\n",
        "**Experiment with the threshold**\n",
        "\n",
        "Consider your dataset size and analysis needs, remember df.info(), can give you a great overview!\n",
        "\n",
        "**If you have more time, try these as well**\n",
        "```python\n",
        "df.dropna(axis=1, how='all')      # Drop columns where ALL values are missing\n",
        "df.dropna(axis=1, how='any')      # Drop columns with ANY missing values\n",
        "df.dropna(axis=0, thresh=5)       # Drop rows with fewer than 5 non-null values\n",
        "df.dropna(subset=['column_name']) # Drop rows with NaN in specific columns\n",
        "```\n",
        "\n",
        "**Important Note:** This creates a new DataFrame (`df_drop`) and doesn't modify the original (`df`). To modify in-place, add `inplace=True`.\n"
      ],
      "metadata": {
        "id": "ngOl28_sERwY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "28o1l7jLPoKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the df['bill_length_mm'] again, do you see some NaNs?"
      ],
      "metadata": {
        "id": "Nn0hRKBKExpx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qaTIoQXdPqEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, instead of dropping these values, you will fill it with the median. To do so, use the function fillna() and inside place the two arguments: *inplace=True* to perform the filling in that column, and *df['bill_length_mm'].median()* to replace NaN with the median of that column.\n",
        "\n",
        "\n",
        "## Filling Missing Values with `.fillna()`\n",
        "\n",
        "### Replacing NaN Values Instead of Dropping Them\n",
        "\n",
        "Sometimes dropping missing data isn't the best solutionâ€”you might lose too much valuable information. Instead, you can **fill** (or **impute**) missing values with reasonable substitutes. The `.fillna()` method gives you flexible options for replacing NaN values.\n",
        "\n",
        "**What does `.fillna()` do?**\n",
        "- Replaces all NaN (missing) values in a column or DataFrame\n",
        "- Allows you to specify what value should replace the missing data\n",
        "- Can modify the original DataFrame or create a new one\n",
        "\n",
        "**Syntax:**\n",
        "```python\n",
        "df['bill_length_mm'].fillna(df['bill_length_mm'].median(), inplace=True)\n",
        "```\n",
        "\n",
        "**Breaking Down the Parameters:**\n",
        "\n",
        "**`df['bill_length_mm'].median()`:**\n",
        "- Calculates the median (middle value) of the column, ignoring NaN values\n",
        "- Uses this calculated value to replace all missing entries\n",
        "- The median is computed from the existing valid data\n",
        "\n",
        "**`inplace=True`:**\n",
        "- Modifies the original DataFrame directly\n",
        "- No need to reassign: `df['column'] = df['column'].fillna(...)`\n",
        "- Changes are permanent (unless you reload the data)\n",
        "- **Without this parameter**, fillna() returns a new Series/DataFrame without modifying the original\n",
        "\n",
        "### Experiment with other values as well,  you can use mean instead of mean (df['bill_length_mm'].mean()) or a single value such as 0"
      ],
      "metadata": {
        "id": "vObqHyHxFG7n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ama2cCj3PrSW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}